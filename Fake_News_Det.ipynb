{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shiva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shiva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dropout, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"C:/Users/shiva/Desktop/Guvi/FakeNews_Det_Capstone/WELFake_Dataset.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "#data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>LAW ENFORCEMENT ON HIGH ALERT Following Threat...</td>\n",
       "      <td>No comment is expected from Barack Obama Membe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Did they post their votes for Hillary already?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>UNBELIEVABLE! OBAMA’S ATTORNEY GENERAL SAYS MO...</td>\n",
       "      <td>Now, most of the demonstrators gathered last ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Bobby Jindal, raised Hindu, uses story of Chri...</td>\n",
       "      <td>A dozen politically active pastors came here f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>SATAN 2: Russia unvelis an image of its terrif...</td>\n",
       "      <td>The RS-28 Sarmat missile, dubbed Satan 2, will...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0           0  LAW ENFORCEMENT ON HIGH ALERT Following Threat...   \n",
       "1           1                                                NaN   \n",
       "2           2  UNBELIEVABLE! OBAMA’S ATTORNEY GENERAL SAYS MO...   \n",
       "3           3  Bobby Jindal, raised Hindu, uses story of Chri...   \n",
       "4           4  SATAN 2: Russia unvelis an image of its terrif...   \n",
       "\n",
       "                                                text  label  \n",
       "0  No comment is expected from Barack Obama Membe...      1  \n",
       "1     Did they post their votes for Hillary already?      1  \n",
       "2   Now, most of the demonstrators gathered last ...      1  \n",
       "3  A dozen politically active pastors came here f...      0  \n",
       "4  The RS-28 Sarmat missile, dubbed Satan 2, will...      1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting swifter\n",
      "  Downloading swifter-1.4.0.tar.gz (1.2 MB)\n",
      "     ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.0/1.2 MB 217.9 kB/s eta 0:00:06\n",
      "     - -------------------------------------- 0.0/1.2 MB 393.8 kB/s eta 0:00:03\n",
      "     -- ------------------------------------- 0.1/1.2 MB 409.6 kB/s eta 0:00:03\n",
      "     --- ------------------------------------ 0.1/1.2 MB 438.1 kB/s eta 0:00:03\n",
      "     --- ------------------------------------ 0.1/1.2 MB 504.4 kB/s eta 0:00:03\n",
      "     ---- ----------------------------------- 0.1/1.2 MB 481.4 kB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 0.2/1.2 MB 492.3 kB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 0.2/1.2 MB 477.7 kB/s eta 0:00:03\n",
      "     ------ --------------------------------- 0.2/1.2 MB 479.2 kB/s eta 0:00:03\n",
      "     ------- -------------------------------- 0.2/1.2 MB 492.1 kB/s eta 0:00:02\n",
      "     -------- ------------------------------- 0.2/1.2 MB 503.2 kB/s eta 0:00:02\n",
      "     --------- ------------------------------ 0.3/1.2 MB 501.4 kB/s eta 0:00:02\n",
      "     --------- ------------------------------ 0.3/1.2 MB 492.0 kB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 0.3/1.2 MB 507.9 kB/s eta 0:00:02\n",
      "     ------------ --------------------------- 0.4/1.2 MB 518.1 kB/s eta 0:00:02\n",
      "     ------------ --------------------------- 0.4/1.2 MB 509.8 kB/s eta 0:00:02\n",
      "     ------------- -------------------------- 0.4/1.2 MB 522.0 kB/s eta 0:00:02\n",
      "     -------------- ------------------------- 0.4/1.2 MB 513.9 kB/s eta 0:00:02\n",
      "     --------------- ------------------------ 0.5/1.2 MB 512.6 kB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 0.5/1.2 MB 522.0 kB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 0.5/1.2 MB 524.6 kB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 0.5/1.2 MB 520.3 kB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 0.5/1.2 MB 498.8 kB/s eta 0:00:02\n",
      "     ------------------ --------------------- 0.6/1.2 MB 503.4 kB/s eta 0:00:02\n",
      "     ------------------- -------------------- 0.6/1.2 MB 503.0 kB/s eta 0:00:02\n",
      "     -------------------- ------------------- 0.6/1.2 MB 513.8 kB/s eta 0:00:02\n",
      "     --------------------- ------------------ 0.6/1.2 MB 506.2 kB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 0.7/1.2 MB 510.0 kB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 0.7/1.2 MB 508.9 kB/s eta 0:00:01\n",
      "     ------------------------ --------------- 0.7/1.2 MB 513.9 kB/s eta 0:00:01\n",
      "     ------------------------ --------------- 0.7/1.2 MB 517.1 kB/s eta 0:00:01\n",
      "     ------------------------- -------------- 0.8/1.2 MB 516.2 kB/s eta 0:00:01\n",
      "     -------------------------- ------------- 0.8/1.2 MB 512.0 kB/s eta 0:00:01\n",
      "     --------------------------- ------------ 0.8/1.2 MB 517.9 kB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 0.8/1.2 MB 521.9 kB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 0.9/1.2 MB 525.6 kB/s eta 0:00:01\n",
      "     ------------------------------ --------- 0.9/1.2 MB 528.1 kB/s eta 0:00:01\n",
      "     ------------------------------- -------- 0.9/1.2 MB 531.5 kB/s eta 0:00:01\n",
      "     ------------------------------- -------- 0.9/1.2 MB 528.0 kB/s eta 0:00:01\n",
      "     --------------------------------- ------ 1.0/1.2 MB 527.8 kB/s eta 0:00:01\n",
      "     --------------------------------- ------ 1.0/1.2 MB 524.3 kB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 1.0/1.2 MB 527.5 kB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 1.1/1.2 MB 531.6 kB/s eta 0:00:01\n",
      "     ------------------------------------ --- 1.1/1.2 MB 528.5 kB/s eta 0:00:01\n",
      "     ------------------------------------- -- 1.1/1.2 MB 527.3 kB/s eta 0:00:01\n",
      "     -------------------------------------- - 1.1/1.2 MB 534.9 kB/s eta 0:00:01\n",
      "     -------------------------------------- - 1.2/1.2 MB 524.4 kB/s eta 0:00:01\n",
      "     ---------------------------------------  1.2/1.2 MB 530.9 kB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.2/1.2 MB 524.7 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: pandas>=1.0.0 in c:\\users\\shiva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from swifter) (2.2.1)\n",
      "Requirement already satisfied: psutil>=5.6.6 in c:\\users\\shiva\\appdata\\roaming\\python\\python312\\site-packages (from swifter) (5.9.8)\n",
      "Collecting dask>=2.10.0 (from dask[dataframe]>=2.10.0->swifter)\n",
      "  Downloading dask-2024.7.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.33.0 in c:\\users\\shiva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from swifter) (4.66.4)\n",
      "Requirement already satisfied: click>=8.1 in c:\\users\\shiva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (8.1.7)\n",
      "Collecting cloudpickle>=1.5.0 (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter)\n",
      "  Downloading cloudpickle-3.0.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting fsspec>=2021.09.0 (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter)\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shiva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (24.0)\n",
      "Collecting partd>=1.4.0 (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter)\n",
      "  Downloading partd-1.4.2-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting pyyaml>=5.3.1 (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter)\n",
      "  Downloading PyYAML-6.0.1-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: toolz>=0.10.0 in c:\\users\\shiva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (0.12.1)\n",
      "Collecting dask-expr<1.2,>=1.1 (from dask[dataframe]>=2.10.0->swifter)\n",
      "  Downloading dask_expr-1.1.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in c:\\users\\shiva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.0.0->swifter) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shiva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.0.0->swifter) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shiva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.0.0->swifter) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\shiva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.0.0->swifter) (2024.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\shiva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.33.0->swifter) (0.4.6)\n",
      "Requirement already satisfied: pyarrow>=7.0.0 in c:\\users\\shiva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask-expr<1.2,>=1.1->dask[dataframe]>=2.10.0->swifter) (15.0.2)\n",
      "Collecting locket (from partd>=1.4.0->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter)\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shiva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->swifter) (1.16.0)\n",
      "Downloading dask-2024.7.1-py3-none-any.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.2 MB 1.3 MB/s eta 0:00:01\n",
      "   - -------------------------------------- 0.0/1.2 MB 653.6 kB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.0/1.2 MB 653.6 kB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.0/1.2 MB 653.6 kB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.0/1.2 MB 653.6 kB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.1/1.2 MB 218.8 kB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.1/1.2 MB 218.6 kB/s eta 0:00:06\n",
      "   --- ------------------------------------ 0.1/1.2 MB 267.9 kB/s eta 0:00:05\n",
      "   --- ------------------------------------ 0.1/1.2 MB 267.9 kB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 0.1/1.2 MB 281.2 kB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.2/1.2 MB 316.5 kB/s eta 0:00:04\n",
      "   ------ --------------------------------- 0.2/1.2 MB 347.1 kB/s eta 0:00:03\n",
      "   ------- -------------------------------- 0.2/1.2 MB 389.8 kB/s eta 0:00:03\n",
      "   -------- ------------------------------- 0.3/1.2 MB 437.2 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 0.4/1.2 MB 518.1 kB/s eta 0:00:02\n",
      "   -------------- ------------------------- 0.4/1.2 MB 598.2 kB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 0.5/1.2 MB 695.1 kB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.6/1.2 MB 731.4 kB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 0.7/1.2 MB 810.6 kB/s eta 0:00:01\n",
      "   ------------------------- -------------- 0.8/1.2 MB 870.3 kB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 0.9/1.2 MB 934.8 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.0/1.2 MB 988.6 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.1/1.2 MB 1.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.1/1.2 MB 1.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.1/1.2 MB 996.2 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.2/1.2 MB 978.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.2/1.2 MB 974.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.2/1.2 MB 974.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.2/1.2 MB 952.6 kB/s eta 0:00:00\n",
      "Downloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Downloading dask_expr-1.1.9-py3-none-any.whl (241 kB)\n",
      "   ---------------------------------------- 0.0/241.9 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 61.4/241.9 kB 1.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 122.9/241.9 kB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 225.3/241.9 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 241.9/241.9 kB 1.5 MB/s eta 0:00:00\n",
      "Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "   ---------------------------------------- 0.0/177.6 kB ? eta -:--:--\n",
      "   ------------------------- -------------- 112.6/177.6 kB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 177.6/177.6 kB 1.8 MB/s eta 0:00:00\n",
      "Downloading partd-1.4.2-py3-none-any.whl (18 kB)\n",
      "Downloading PyYAML-6.0.1-cp312-cp312-win_amd64.whl (138 kB)\n",
      "   ---------------------------------------- 0.0/138.7 kB ? eta -:--:--\n",
      "   -------------------------- ------------- 92.2/138.7 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 138.7/138.7 kB 1.6 MB/s eta 0:00:00\n",
      "Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Building wheels for collected packages: swifter\n",
      "  Building wheel for swifter (setup.py): started\n",
      "  Building wheel for swifter (setup.py): finished with status 'done'\n",
      "  Created wheel for swifter: filename=swifter-1.4.0-py3-none-any.whl size=16514 sha256=aab8086ba5ae3726313c76309888648d9dc19bbaa6de80736b81d7dcecd36927\n",
      "  Stored in directory: c:\\users\\shiva\\appdata\\local\\pip\\cache\\wheels\\d9\\31\\ff\\ff51141a088571a9f672449e5aad5ea8bb35ca5d95ba135f30\n",
      "Successfully built swifter\n",
      "Installing collected packages: pyyaml, locket, fsspec, cloudpickle, partd, dask, dask-expr, swifter\n",
      "Successfully installed cloudpickle-3.0.0 dask-2024.7.1 dask-expr-1.1.9 fsspec-2024.6.1 locket-1.0.0 partd-1.4.2 pyyaml-6.0.1 swifter-1.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    try:\n",
    "        text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [word.lower() for word in tokens]\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        return ' '.join(tokens)\n",
    "    except TypeError:\n",
    "        return ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shiva\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Pandas Apply: 100%|██████████| 72134/72134 [02:45<00:00, 436.99it/s]\n"
     ]
    }
   ],
   "source": [
    "import swifter\n",
    "data['clean_text'] = data['text'].swifter.apply(preprocess_text)\n",
    "data['clean_title_text'] = data['title'] + \" \" + data['clean_text']\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shiva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shiva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "file_path = \"C:/Users/shiva/Desktop/Guvi/FakeNews_Det_Capstone/WELFake_Dataset.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "data.head()\n",
    "data['text'] = data['text'].astype(str)\n",
    "data['title'] = data['title'].astype(str)\n",
    "data.fillna('', inplace=True)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "data['clean_text'] = data['text'].apply(preprocess_text)\n",
    "data['clean_title'] = data['title'].apply(preprocess_text)\n",
    "data['clean_title_text'] = data['clean_title'] + \" \" + data['clean_text']\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(data['clean_title_text'])\n",
    "sequences = tokenizer.texts_to_sequences(data['clean_title_text'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=500)\n",
    "\n",
    "X = padded_sequences\n",
    "y = data['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shiva\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m902/902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m687s\u001b[0m 746ms/step - accuracy: 0.8642 - loss: 0.3165 - val_accuracy: 0.9306 - val_loss: 0.1795\n",
      "Epoch 2/5\n",
      "\u001b[1m902/902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m684s\u001b[0m 759ms/step - accuracy: 0.9410 - loss: 0.1635 - val_accuracy: 0.9496 - val_loss: 0.1558\n",
      "Epoch 3/5\n",
      "\u001b[1m902/902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m679s\u001b[0m 753ms/step - accuracy: 0.9534 - loss: 0.1357 - val_accuracy: 0.8452 - val_loss: 0.2665\n",
      "Epoch 4/5\n",
      "\u001b[1m902/902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m709s\u001b[0m 787ms/step - accuracy: 0.9558 - loss: 0.1238 - val_accuracy: 0.9574 - val_loss: 0.1209\n",
      "Epoch 5/5\n",
      "\u001b[1m902/902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m717s\u001b[0m 795ms/step - accuracy: 0.9749 - loss: 0.0766 - val_accuracy: 0.9713 - val_loss: 0.0845\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=5000, output_dim=128, input_length=500))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m451/451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 113ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      7089\n",
      "           1       0.97      0.97      0.97      7338\n",
      "\n",
      "    accuracy                           0.97     14427\n",
      "   macro avg       0.97      0.97      0.97     14427\n",
      "weighted avg       0.97      0.97      0.97     14427\n",
      "\n",
      "F1-Score: 0.9717983651226159\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"F1-Score:\", f1_score(y_test, y_pred))\n",
    "\n",
    "model.save('fake_news_classifier.h5')\n",
    "\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flask_ngrok"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading flask_ngrok-0.0.25-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting Flask>=0.8 (from flask_ngrok)\n",
      "  Downloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\shiva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from flask_ngrok) (2.31.0)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in c:\\users\\shiva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from Flask>=0.8->flask_ngrok) (3.0.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in c:\\users\\shiva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from Flask>=0.8->flask_ngrok) (3.1.3)\n",
      "Collecting itsdangerous>=2.1.2 (from Flask>=0.8->flask_ngrok)\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\users\\shiva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from Flask>=0.8->flask_ngrok) (8.1.7)\n",
      "Requirement already satisfied: blinker>=1.6.2 in c:\\users\\shiva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from Flask>=0.8->flask_ngrok) (1.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shiva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->flask_ngrok) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shiva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->flask_ngrok) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shiva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->flask_ngrok) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shiva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->flask_ngrok) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\shiva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click>=8.1.3->Flask>=0.8->flask_ngrok) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shiva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from Jinja2>=3.1.2->Flask>=0.8->flask_ngrok) (2.1.5)\n",
      "Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
      "Downloading flask-3.0.3-py3-none-any.whl (101 kB)\n",
      "   ---------------------------------------- 0.0/101.7 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 10.2/101.7 kB ? eta -:--:--\n",
      "   --------------- ----------------------- 41.0/101.7 kB 487.6 kB/s eta 0:00:01\n",
      "   --------------- ----------------------- 41.0/101.7 kB 487.6 kB/s eta 0:00:01\n",
      "   ----------------------- --------------- 61.4/101.7 kB 409.6 kB/s eta 0:00:01\n",
      "   ----------------------- --------------- 61.4/101.7 kB 409.6 kB/s eta 0:00:01\n",
      "   ----------------------------------- --- 92.2/101.7 kB 350.1 kB/s eta 0:00:01\n",
      "   -------------------------------------- 101.7/101.7 kB 344.0 kB/s eta 0:00:00\n",
      "Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: itsdangerous, Flask, flask_ngrok\n",
      "Successfully installed Flask-3.0.3 flask_ngrok-0.0.25 itsdangerous-2.2.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install flask_ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shiva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shiva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 956ms/step\n",
      "Article: This is a piece of fake news about a celebrity scandal.\n",
      "Prediction: real\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "Article: The stock market reached an all-time high today due to strong economic growth.\n",
      "Prediction: real\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "Article: Unbelievable! Aliens have landed on Earth and are communicating with humans.\n",
      "Prediction: real\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "model_path = 'fake_news_classifier.h5'\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def predict(news_text):\n",
    "    clean_news = preprocess_text(news_text)\n",
    "    seq = tokenizer.texts_to_sequences([clean_news])\n",
    "    padded = pad_sequences(seq, maxlen=500)\n",
    "    prediction = (model.predict(padded) > 0.5).astype(\"int32\")[0][0]\n",
    "    return 'real' if prediction == 1 else 'fake'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_articles = [\n",
    "        \"This is a piece of fake news about a celebrity scandal.\",\n",
    "        \"The stock market reached an all-time high today due to strong economic growth.\",\n",
    "        \"Unbelievable! Aliens have landed on Earth and are communicating with humans.\"\n",
    "    ]\n",
    "\n",
    "    for article in test_articles:\n",
    "        result = predict(article)\n",
    "        print(f\"Article: {article}\\nPrediction: {result}\\n\")\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
